# Paper Summary: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)

## Abstract
- TBD

## Introduction
- TBD

## Approach
  - similar approach for GPT3/PaLM
  - inspired by the Chinchilla scaling laws
  - large transformers trained on large corpora with standard optimizer
### Pre-training Data
- publicly available & open-source-compatible data only
- reuse data sources leveraged for existing LLMs

<p align="center"><img src="../images/LLaMA_tbl_01.png" alt="Pre-training data" width="400"/></p>

  - English CommonCrawl [67%]
    - TBD
  - C4 [15%]
    - TBD
  - Github [4.5%]
    - TBD
  - Wikipedia [4.5%]
    - TBD
  - Gutenberg and Books3 [4.5%]
    - TBD
  - ArXiv [2.5%]
    - TBD
  - Stack Exchange [2%]
    - TBD
  - Tokenizer
    - TBD
### Architecture
- TBD (table 2)
  - Pre-normalization [GPT3]
    - TBD
  - SwiGLU activation function [PaLM]
    - TBD
  - Rotary Embeddings [GPTNeo]
    - TBD
### Optimizer
- TBD
### Efficient implementation
- TBD

## Main results
- TBD
### Common Sense Reasoning
- TBD (table 3)
### Closed-book Question Answering
- TBD (table 4 & 5)
### Reading Comprehension
- TBD (table 6)
### Mathematical reasoning
- TBD (table 7)
### Code generation
- TBD (table 8)
### Massive Multitask Language Understanding
- TBD (table 9)
### Evolution of performance during training
- TBD (figure 1 & 2)

## Instruction Finetuning
- TBD (table 10)

## Bias, Toxicity and Misinformation
- TBD
### RealToxicityPrompts
- TBD (table 11)
### CrowS-Pairs
- TBD (table 12)
### WinoGender
- TBD (table 13)
### TruthfulQA
- TBD (table 14)

## Carbon footprint
- TBD (table 15)

## Related work
  - Language models
    - TBD
  - Architecture
    - TBD
  - Scaling
    - TBD

## Conclusion
- TBD