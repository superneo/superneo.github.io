# paper summary: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

This is a simple coverage for the transformer paper.<br>
It'd be a brief and supplementary summary of the original,<br>
not a kind of line-by-line commentary or analysis.<br>

## model architecture

### encoder-decoder paring in brief
<img src="../images/transformer/fig_01.png" alt="original paper figure 1" width="300"/>

